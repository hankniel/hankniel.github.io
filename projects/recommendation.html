{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c50928",
   "metadata": {},
   "source": [
    "### Context\n",
    "During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.  \n",
    "\n",
    "In a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).  \n",
    "\n",
    "Recommender systems are really critical in some industries as they can generate a huge amount of income when they are efficient or also be a way to stand out significantly from competitors. As a proof of the importance of recommender systems, we can mention that, a few years ago, Netflix organised a challenges (the “Netflix prize”) where the goal was to produce a recommender system that performs better than its own algorithm with a prize of 1 million dollars to win.  \n",
    "\n",
    "**(Baptiste Rocca - Towards Data Science)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0692f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffb5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SparkSession, pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"test\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650bbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dccdea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "schema = StructType([\n",
    "    StructField(\"userID\", IntegerType(), False),\n",
    "    StructField(\"animeID\", IntegerType(), False),\n",
    "    StructField(\"rating\", IntegerType(), True)\n",
    "])\n",
    "data = spark.read.csv(\"C:/Users/admin/OneDrive/Máy tính/Python Project/Ratings.csv\", header=True, schema=schema).limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe410d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userID|animeID|rating|\n",
      "+------+-------+------+\n",
      "|     0|    430|     9|\n",
      "|     0|   1004|     5|\n",
      "|     0|   3010|     7|\n",
      "|     0|    570|     7|\n",
      "|     0|   2762|     9|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7b4624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userID|animeID|rating|\n",
      "+------+-------+------+\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(\"userID IS NULL OR animeID IS NULL OR rating IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95674779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- animeID: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dd10c",
   "metadata": {},
   "source": [
    "- The original data is already in a long table format, which is required for ALS to work.\n",
    "- The userID and animeID are integer.\n",
    "- There are no missing values.  \n",
    "--> The data is clean and ready to fit into the ALS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d19d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0.9532190005707282)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating Sparsity (Sparsity = Number of ratings in Matrix/Size of the Matrix)\n",
    "#Number of Ratings in Matrix\n",
    "numerator = data.count()\n",
    "\n",
    "#Size of the Matrix\n",
    "users = data.select(\"userID\").distinct().count()\n",
    "anime = data.select(\"animeID\").distinct().count()\n",
    "denominator = users*anime\n",
    "\n",
    "#Sparsity\n",
    "sparsity = 1 - (numerator*1.0/denominator)\n",
    "print(\"Sparsity: \"), sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f06e0",
   "metadata": {},
   "source": [
    "The matrix has Sparsity approximately = 1, meaning that the matrix has many missing values, which is normal since a user can only watch a small proportion of the total anime available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da5e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 1.9024431585514359)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BUILDING ALS MODEL\n",
    "#Split data\n",
    "(train, test) = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "#Evaluation using RMSE\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "#ALS model\n",
    "als = ALS(userCol='userID', itemCol='animeID', ratingCol='rating',\n",
    "         coldStartStrategy='drop', nonnegative=True, implicitPrefs=False,\n",
    "         rank=10, maxIter=10, regParam=0.1) #The Ratings of anime are explicit ratings\n",
    "model = als.fit(train)\n",
    "\n",
    "#Predict\n",
    "predictions = model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "#Print result\n",
    "print(\"RMSE = \"), rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56105d",
   "metadata": {},
   "source": [
    "- RMSE is approximately 1.9, indicating a difference of about 1.9 points in predictions on a 10-point scale, which is not a bad score.  \n",
    "- Since I am running pyspark on a local machine, the computer's performance is not enough to handle large amounts of data, so I can only use a limited amount of data. The more data there is, the better the model's performance will be.  \n",
    "- Also, If we can use CrossValidator to tune hyperparameters then the model's performance will improve even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b87ccb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userID|     recommendations|\n",
      "+------+--------------------+\n",
      "|    20|[{9253, 10.347785...|\n",
      "|    40|[{846, 10.382234}...|\n",
      "|    10|[{918, 9.427543},...|\n",
      "|    50|[{9756, 10.745759...|\n",
      "|    70|[{2001, 10.061969...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ouput recommendations\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e4536",
   "metadata": {},
   "source": [
    "Here we can see that the output is hard to read so we will need to make the output more human-readable by cleaning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65e4126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|userID|animeID|prediction|\n",
      "+------+-------+----------+\n",
      "|    20|   9253| 10.347785|\n",
      "|    20|   2001| 10.281152|\n",
      "|    20|  36466|  9.983582|\n",
      "|    20|    245|  9.964258|\n",
      "|    20|   4565|  9.913297|\n",
      "|    20|    457|  9.899969|\n",
      "|    20|  28851|  9.848605|\n",
      "|    20|    552|  9.800172|\n",
      "|    20|  39533|  9.732345|\n",
      "|    20|  13125|  9.716911|\n",
      "+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cleaning recommendations output\n",
    "userRecs.registerTempTable(\"userRecs_temp\")\n",
    "\n",
    "clean_Recs = spark.sql(\"SELECT userID, animeID_and_rating.animeID AS animeID, animeID_and_rating.rating AS prediction\\\n",
    "                       FROM userRecs_temp\\\n",
    "                       LATERAL VIEW explode(recommendations) exploded_table\\\n",
    "                       AS animeID_and_rating\")\n",
    "clean_Recs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff78d",
   "metadata": {},
   "source": [
    "- Here the result is readable and looks neat. However, we can see that in the first 2 records the model is giving a predicted rating that is higher than the rating range (0-10). However, as far as I know, this is not the model's error and this could give us extra information about how much stronger than 10 the predicted rating is. Based on analyzing records with scores higher than 10 or lower than 0 (if any), we can find more insights about each user's preferences.  \n",
    "\n",
    "- The next problem is that the recommendations include the anime that has been watched. Since the ALS factorization the matrix into 2 matrices with full values, it will recommend all animes. We must filter anime that has been watched by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de271eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+------+\n",
      "|userID|animeID|prediction|rating|\n",
      "+------+-------+----------+------+\n",
      "|    20|   9253| 10.347785|  null|\n",
      "|    20|   2001| 10.281152|  null|\n",
      "|    20|    245|  9.964258|  null|\n",
      "|    20|   4565|  9.913297|  null|\n",
      "|    20|    457|  9.899969|  null|\n",
      "|    20|  28851|  9.848605|  null|\n",
      "|    20|  13125|  9.716911|  null|\n",
      "|    40|    846| 10.382234|  null|\n",
      "|    40|   9253|  10.23476|  null|\n",
      "|    40|   3702| 10.105054|  null|\n",
      "+------+-------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_Recs.join(data, ['userID', 'animeID'], 'left').filter(data.rating.isNull()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483ba14",
   "metadata": {},
   "source": [
    "So we have the complete results of each user's predicted rating as well as we can arbitrarily view the top n anime series that the system recommends for each user. Building an effective model depends on the input data and the process of cleaning the data as well as carefully tuning the model.  \n",
    "\n",
    "Also note that this small project only covers collaborative filtering. There are also content-based filtering for the recommendation engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
